{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:15.126068Z",
     "start_time": "2021-02-16T02:46:14.467436Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:15.128796Z",
     "start_time": "2021-02-16T02:46:15.127195Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:16.057544Z",
     "start_time": "2021-02-16T02:46:15.129999Z"
    }
   },
   "outputs": [],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug-ADE raw file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T03:01:09.926598Z",
     "start_time": "2021-02-16T02:59:42.001833Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "\n",
    "cd data\n",
    "# 25M\n",
    "wget https://fis.fda.gov/content/Exports/faers_ascii_2013q1.zip\n",
    "# 30M\n",
    "wget https://fis.fda.gov/content/Exports/faers_ascii_2014q1.zip\n",
    "\n",
    "\n",
    "for f in `ls -1 *.zip`; \n",
    "do unzip $f -d `basename $f .zip`; \n",
    "done\n",
    "\n",
    "# rm faers_ascii_2013q1.zip\n",
    "# rm faers_ascii_2014q1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T03:03:18.589626Z",
     "start_time": "2021-02-16T03:03:17.422879Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_drug = pd.read_csv(\"data/faers_ascii_2013q1/ascii/DRUG13Q1.txt\", sep=\"$\")\n",
    "tmp_reac = pd.read_csv(\"data/faers_ascii_2013q1/ascii/REAC13Q1.txt\", sep=\"$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:49.772586Z",
     "start_time": "2021-02-16T02:46:49.752575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing: only use 1000 rows\n",
    "tmp_drug = tmp_drug[:1000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:51.976169Z",
     "start_time": "2021-02-16T02:46:51.879504Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dataset = tmp_drug.merge(tmp_reac, on=\"primaryid\", how=\"inner\")\n",
    "\n",
    "tmp_dataset = tmp_dataset[[\"primaryid\", 'drugname', 'role_cod', 'pt']].copy()\n",
    "\n",
    "tmp_dataset['drugname'] = tmp_dataset['drugname'].str.lower()\n",
    "tmp_dataset['pt'] = tmp_dataset['pt'].str.lower()\n",
    "\n",
    "tmp_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:52.935062Z",
     "start_time": "2021-02-16T02:46:52.932520Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:46:54.923411Z",
     "start_time": "2021-02-16T02:46:54.914474Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dataset[tmp_dataset.primaryid == 30375293]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption #1: for each `primaryid`, there is no difference in `pt`, i.e., if one drug is `PS` for one `pt`, it is also `PS` for all other `pt`s; same as `SS` and `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:47:21.383148Z",
     "start_time": "2021-02-16T02:47:21.378438Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dataset[tmp_dataset.primaryid == 30375293].groupby(\"drugname\")['pt'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:49:07.297244Z",
     "start_time": "2021-02-16T02:49:07.293866Z"
    }
   },
   "outputs": [],
   "source": [
    "ret_dict = {\"primaryid\":[], 'pt':[], 'drug':[], 'role_cod':[]}\n",
    "\n",
    "def collect_all(df, ret, rand_seed):\n",
    "    # df is already grouped by 'primaryid'\n",
    "    # to be used with DataFrame.apply()\n",
    "    \n",
    "    ret['primaryid'].append(df.name)\n",
    "    \n",
    "    ret['pt'].append(df['pt'].unique().tolist())\n",
    "    \n",
    "    tmp = df[['drugname','role_cod']].copy()\n",
    "    \n",
    "    # NOTE: assumption made: for one drug, there is ONLY ONE role_cod\n",
    "    # NOTE update: the above assumption is not true... most of times..\n",
    "    #              so, for drugname with multiple roles, pick the first one in \"drug_seq\"\n",
    "    #              This method is heavily depending on the fact that raw data is ordered by drug_seq!\n",
    "    # tmp = tmp[~tmp[['drugname']].duplicated(keep=\"first\")]\n",
    "    \n",
    "    tmp.drop_duplicates(['drugname'], keep='first', inplace=True)\n",
    "    \n",
    "    # shuffle tmp -> in case PS always comes first..\n",
    "    tmp = tmp.sample(frac=1, random_state=rand_seed)\n",
    "    \n",
    "    ret['drug'].append(tmp['drugname'].tolist())\n",
    "    ret['role_cod'].append(tmp['role_cod'].tolist())\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:49:09.288997Z",
     "start_time": "2021-02-16T02:49:09.128341Z"
    }
   },
   "outputs": [],
   "source": [
    "param_seed = 2991\n",
    "tmp_dataset.groupby(\"primaryid\").apply(collect_all, ret=ret_dict, rand_seed=param_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:49:17.239752Z",
     "start_time": "2021-02-16T02:49:17.236319Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "for k,v in ret_dict.items():\n",
    "    print(k, \": \", v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process through PubMedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:50:01.901929Z",
     "start_time": "2021-02-16T02:49:58.512964Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")  \n",
    "pubmed_bert = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:54:05.669956Z",
     "start_time": "2021-02-16T02:54:05.668100Z"
    }
   },
   "outputs": [],
   "source": [
    "# max length of one WordPiece-tokenized report (for truncating and padding)\n",
    "# This may not be long enough for some reports\n",
    "param_max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:54:08.850069Z",
     "start_time": "2021-02-16T02:54:08.837211Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoding\n",
    "all_encodings = tokenizer(ret_dict['drug'], ret_dict['pt'], max_length=param_max_length, padding=\"max_length\", truncation=True, \n",
    "                is_split_into_words=True, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:55:03.611488Z",
     "start_time": "2021-02-16T02:55:03.609087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drug Rold Codes\n",
    "unique_roles = set([\"PS\", \"SS\", \"C\", \"I\"])\n",
    "\n",
    "# hard code role_cod to numeric categories\n",
    "role_2_id = {\"PS\":0, \"SS\":1, \"C\":2, \"I\":3}\n",
    "id_2_role = {value:key for key, value in role_2_id.items()}\n",
    "\n",
    "\n",
    "n_roles = len(unique_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:55:03.782979Z",
     "start_time": "2021-02-16T02:55:03.780658Z"
    }
   },
   "outputs": [],
   "source": [
    "print(role_2_id)\n",
    "print(id_2_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:55:21.296950Z",
     "start_time": "2021-02-16T02:55:21.272128Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "for report_labels, report_offset in zip(ret_dict['role_cod'], all_encodings['offset_mapping']):\n",
    "    report_labels_num = [role_2_id[_] for _ in report_labels]\n",
    "    # NOTE: -999 is arbitrary here...\n",
    "    report_long_labels = np.ones(len(report_offset),dtype=int) * -999\n",
    "    \n",
    "    arr_offset = np.array(report_offset)\n",
    "    \n",
    "    # positions of special character [CLS] drug [SEP] ADE [SEP]\n",
    "    tmp = np.where((arr_offset[:,0] == 0) & (arr_offset[:,1] == 0))\n",
    "    \n",
    "    # find positions of labels whose first offset position is 0 and the second is not 0\n",
    "    # in drug part. Set those in ADE part to false, i.e., keep them -999 -> no use for predictions\n",
    "    is_start = ((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0))\n",
    "    is_start[tmp[0][1]:] = False\n",
    "    \n",
    "    # sum(is_start): in case of truncation, where drug got truncated, if so, also truncate labels\n",
    "    report_long_labels[is_start] = report_labels_num[:sum(is_start)]\n",
    "    encoded_labels.append(report_long_labels.tolist())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:55:47.090668Z",
     "start_time": "2021-02-16T02:55:47.088102Z"
    }
   },
   "outputs": [],
   "source": [
    "# This could be casted to all samples\n",
    "# use position 1\n",
    "tmp_pos = torch.tensor([1] * param_max_length).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:54.336450Z",
     "start_time": "2021-02-12T15:58:54.250361Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:54.430473Z",
     "start_time": "2021-02-12T15:58:54.339221Z"
    }
   },
   "outputs": [],
   "source": [
    "param_fine_tune_hidden_size = 512\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32 # 32\n",
    "\n",
    "param_device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze PubMedBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:54.522795Z",
     "start_time": "2021-02-12T15:58:54.434118Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in pubmed_bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:57.440464Z",
     "start_time": "2021-02-12T15:58:54.526573Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(torch.tensor(all_encodings['input_ids']), torch.tensor(all_encodings['attention_mask']),\n",
    "                           torch.tensor(all_encodings['token_type_ids']), torch.tensor(encoded_labels))\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:57.572665Z",
     "start_time": "2021-02-12T15:58:57.441614Z"
    }
   },
   "outputs": [],
   "source": [
    "class Fine_Tune_Multilevel(nn.Module):\n",
    "\n",
    "    def __init__(self, bert, hidden_size, category_size):\n",
    "      \n",
    "        super(Fine_Tune_Multilevel, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "      \n",
    "        param_bert_size = pubmed_bert.config.hidden_size\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(param_bert_size, hidden_size) #(768, 512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(hidden_size, category_size)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, attention_mask, token_type_ids, position_ids):\n",
    "\n",
    "        #pass the inputs to the model\n",
    "        ## NOTE: this unpacking may not work...\n",
    "        ret_bert = self.bert(sent_id, attention_mask=attention_mask, \n",
    "                              token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "\n",
    "        x = self.fc1(ret_bert[0])\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:59.333365Z",
     "start_time": "2021-02-12T15:58:57.575879Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fine_tune = Fine_Tune_Multilevel(pubmed_bert, param_fine_tune_hidden_size, n_roles)\n",
    "model_fine_tune = model_fine_tune.to(param_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:59.338153Z",
     "start_time": "2021-02-12T15:58:59.334334Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model_fine_tune.parameters(),\n",
    "                  lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:59.480178Z",
     "start_time": "2021-02-12T15:58:59.339043Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "# cross_entropy  = nn.NLLLoss() \n",
    "cross_entropy = nn.NLLLoss()\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-12T15:58:59.565516Z",
     "start_time": "2021-02-12T15:58:59.483102Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, positions, n_categories):\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    \n",
    "    positions = positions.to(device)\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        model_ret = model(sent_id=batch[0], attention_mask=batch[1],\n",
    "                           token_type_ids=batch[2], position_ids=positions)\n",
    "\n",
    "        # calculate loss: remove labels with -999\n",
    "        preds = model_ret.view(-1, n_categories)\n",
    "        target = batch[3].flatten()\n",
    "\n",
    "        loss = cross_entropy(preds[target != -999], target[target != -999])\n",
    "\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push itk to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.484Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "# valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train(model=model_fine_tune, device=param_device, train_dataloader=train_dataloader,\n",
    "                             positions=tmp_pos, n_categories=n_roles)\n",
    "    \n",
    "    #evaluate model\n",
    "    # valid_loss, _ = evaluate()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    # valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    # print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future use of loooooong run: store output(print)\n",
    "\n",
    "%%capture stored_output   \n",
    "stored_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on training performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.491Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.494Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.pointplot(x=np.arange(len(train_losses)),y=train_losses)\n",
    "ax.set(xlabel='Training Epochs', ylabel='Training Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.500Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on training performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.505Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_model = model_fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.508Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_model = tmp_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.511Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_model = tmp_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.514Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 10\n",
    "tmp_ret = tmp_model(torch.tensor(all_encodings['input_ids'])[:i], torch.tensor(all_encodings['attention_mask'])[:i],\n",
    "                           torch.tensor(all_encodings['token_type_ids'])[:i], tmp_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.518Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for preds, labels, primaryid, enc_drugs, pt, raw_drugs, offset_mappings in zip(tmp_ret.argmax(dim=2), torch.tensor(encoded_labels)[:i], ret_dict['primaryid'][:i], all_encodings['input_ids'][:i], ret_dict['pt'][:i],ret_dict['drug'][:i], all_encodings['offset_mapping'][:i]):\n",
    "    print(primaryid, \" pt list: \", pt)\n",
    "    \n",
    "    token_drugs = tokenizer.convert_ids_to_tokens(enc_drugs)\n",
    "    print(\"drug list: \", raw_drugs)\n",
    "    counter_sep = 0\n",
    "    for _pred,_label,_drug, _offset in zip(preds, labels, token_drugs, offset_mappings):\n",
    "        if _drug == \"[SEP]\":\n",
    "            counter_sep += 1\n",
    "            \n",
    "        txt_prt = \"{:>12} {:<10} {:<5d} {:<5d}\".format(primaryid, _drug, _pred, _label)\n",
    "        \n",
    "        if (_offset[0] == 0) and (_offset[1] != 0) and (counter_sep < 1):\n",
    "            print('\\033[91m', txt_prt)\n",
    "        else:\n",
    "            print('\\033[90m',txt_prt)\n",
    "        \n",
    "        if counter_sep == 2:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.521Z"
    }
   },
   "outputs": [],
   "source": [
    "print(id_2_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:01:53.005565Z",
     "start_time": "2021-02-16T02:01:51.674068Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_drug = pd.read_csv(\"data/faers_ascii_2014q1/ascii/DRUG14Q1.txt\", sep=\"$\")\n",
    "tmp_reac = pd.read_csv(\"data/faers_ascii_2014q1/ascii/REAC14Q1.txt\", sep=\"$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:02:03.997181Z",
     "start_time": "2021-02-16T02:01:57.847460Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dataset_eval = tmp_drug.merge(tmp_reac, on=\"primaryid\", how=\"inner\")\n",
    "\n",
    "tmp_dataset_eval = tmp_dataset_eval[[\"primaryid\", 'drugname', 'role_cod', 'pt']].copy()\n",
    "\n",
    "tmp_dataset_eval['drugname'] = tmp_dataset_eval['drugname'].str.lower()\n",
    "tmp_dataset_eval['pt'] = tmp_dataset_eval['pt'].str.lower()\n",
    "\n",
    "tmp_dataset_eval.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select top 100,000 rows for evaluation here\n",
    "tmp_dataset_eval = tmp_dataset_eval[:100000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:02:03.999891Z",
     "start_time": "2021-02-16T02:02:03.998207Z"
    }
   },
   "outputs": [],
   "source": [
    "ret_dict_eval = {\"primaryid\":[], 'pt':[], 'drug':[], 'role_cod':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:08:17.661448Z",
     "start_time": "2021-02-16T02:02:04.000879Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dataset_eval.groupby(\"primaryid\").apply(collect_all, ret=ret_dict_eval, rand_seed=param_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:08:33.278051Z",
     "start_time": "2021-02-16T02:08:17.662605Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_encodings = tokenizer(ret_dict_eval['drug'], ret_dict_eval['pt'], max_length=param_max_length, padding=\"max_length\", truncation=True, \n",
    "                is_split_into_words=True, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:09:20.416518Z",
     "start_time": "2021-02-16T02:08:33.279151Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_labels_eval = []\n",
    "for report_labels, report_offset in zip(ret_dict_eval['role_cod'], eval_encodings['offset_mapping']):\n",
    "    report_labels_num = [role_2_id[_] for _ in report_labels]\n",
    "    # NOTE: -999 is arbitrary here...\n",
    "    report_long_labels = np.ones(len(report_offset),dtype=int) * -999\n",
    "    \n",
    "    arr_offset = np.array(report_offset)\n",
    "    \n",
    "    # positions of special character [CLS] drug [SEP] ADE [SEP]\n",
    "    tmp = np.where((arr_offset[:,0] == 0) & (arr_offset[:,1] == 0))\n",
    "    \n",
    "    # find positions of labels whose first offset position is 0 and the second is not 0\n",
    "    # in drug part. Set those in ADE part to false, i.e., keep them -999 -> no use for predictions\n",
    "    is_start = ((arr_offset[:,0] == 0) & (arr_offset[:,1] != 0))\n",
    "    is_start[tmp[0][1]:] = False\n",
    "    \n",
    "    # sum(is_start): in case of truncation, where drug got truncated, if so, also truncate labels\n",
    "    report_long_labels[is_start] = report_labels_num[:sum(is_start)]\n",
    "    encoded_labels_eval.append(report_long_labels.tolist())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.607Z"
    }
   },
   "outputs": [],
   "source": [
    "# test on one report..\n",
    "ii = 0\n",
    "i =21\n",
    "with torch.no_grad():\n",
    "    eval_ret = tmp_model(torch.tensor(eval_encodings['input_ids'])[ii:i], torch.tensor(eval_encodings['attention_mask'])[ii:i],\n",
    "                           torch.tensor(eval_encodings['token_type_ids'])[ii:i], tmp_pos)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T02:09:23.686643Z",
     "start_time": "2021-02-16T02:09:20.417548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply on all evaluation set\n",
    "# wrap tensors\n",
    "eval_data = TensorDataset(torch.tensor(eval_encodings['input_ids']), torch.tensor(eval_encodings['attention_mask']),\n",
    "                           torch.tensor(eval_encodings['token_type_ids']), torch.tensor(encoded_labels_eval))\n",
    "\n",
    "\n",
    "# dataLoader for train set\n",
    "eval_dataloader = DataLoader(eval_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-16T02:02:22.626Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: running this on CPU takes tooo long (~30 min). Should better move to GPU\n",
    "\n",
    "eval_ret = []\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    tmp_ret = tmp_model(sent_id=batch[0], attention_mask=batch[1],\n",
    "                           token_type_ids=batch[2], position_ids=tmp_pos)\n",
    "    \n",
    "    eval_ret.extend(tmp_ret.tolist())\n",
    "\n",
    "eval_ret = torch.tensor(eval_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-16T02:09:54.305Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_ret.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-16T02:09:55.077Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor(encoded_labels_eval).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-16T02:09:55.495Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_ret.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-16T02:09:57.419Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = eval_ret.view(-1, n_roles)\n",
    "target = torch.tensor(encoded_labels_eval).flatten()\n",
    "\n",
    "loss = cross_entropy(preds[target != -999], target[target != -999])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-12T15:52:33.634Z"
    }
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:41:25.997515Z",
     "start_time": "2021-02-09T00:41:25.902062Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.pointplot(x=np.arange(len(train_losses)),y=train_losses)\n",
    "# ax.axhline(loss.item(), ls='--', c=\"red\")\n",
    "ax.scatter(9, loss.item(), marker=\"o\",c=\"red\")\n",
    "ax.set(xlabel='Training Epochs', ylabel='Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:43:54.431035Z",
     "start_time": "2021-02-09T00:43:52.066523Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter_prt = 0\n",
    "\n",
    "for preds, labels, primaryid, enc_drugs, pt, raw_drugs, offset_mappings in zip(eval_ret.argmax(dim=2), torch.tensor(encoded_labels_eval)[:i], ret_dict_eval['primaryid'][:i], eval_encodings['input_ids'][:i], ret_dict_eval['pt'][:i],ret_dict_eval['drug'][:i], eval_encodings['offset_mapping'][:i]):\n",
    "    print(primaryid, \" pt list: \", pt)\n",
    "    \n",
    "    token_drugs = tokenizer.convert_ids_to_tokens(enc_drugs)\n",
    "    print(\"drug list: \", raw_drugs)\n",
    "    counter_sep = 0\n",
    "    for _pred,_label,_drug, _offset in zip(preds, labels, token_drugs, offset_mappings):\n",
    "        if _drug == \"[SEP]\":\n",
    "            counter_sep += 1\n",
    "            \n",
    "        txt_prt = \"{:>12} {:<10} {:<5d} {:<5d}\".format(primaryid, _drug, _pred, _label)\n",
    "        \n",
    "        if (_offset[0] == 0) and (_offset[1] != 0) and (counter_sep < 1):\n",
    "            print('\\033[91m', txt_prt)\n",
    "        else:\n",
    "            print('\\033[90m',txt_prt)\n",
    "        \n",
    "        if counter_sep == 2:\n",
    "            break\n",
    "        \n",
    "    counter_prt += 1\n",
    "    if counter_prt == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:46:46.193670Z",
     "start_time": "2021-02-09T00:46:17.936696Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "target_ls = []\n",
    "for preds, labels, primaryid, enc_drugs, pt, raw_drugs, offset_mappings in zip(eval_ret.argmax(dim=2), torch.tensor(encoded_labels_eval), ret_dict_eval['primaryid'], eval_encodings['input_ids'], ret_dict_eval['pt'],ret_dict_eval['drug'], eval_encodings['offset_mapping']):\n",
    "    \n",
    "    \n",
    "    token_drugs = tokenizer.convert_ids_to_tokens(enc_drugs)\n",
    "\n",
    "    counter_sep = 0\n",
    "    for _pred,_label,_drug, _offset in zip(preds, labels, token_drugs, offset_mappings):\n",
    "        if _drug == \"[SEP]\":\n",
    "            counter_sep += 1\n",
    "            \n",
    "        \n",
    "        \n",
    "        if (_offset[0] == 0) and (_offset[1] != 0) and (counter_sep < 1):\n",
    "            preds_ls.append(_pred.item())\n",
    "            target_ls.append(_label.item())\n",
    "        \n",
    "        \n",
    "        if counter_sep == 2:\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:46:46.550112Z",
     "start_time": "2021-02-09T00:46:46.194673Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(zip(preds_ls, target_ls), columns=['pred', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:46:46.575208Z",
     "start_time": "2021-02-09T00:46:46.551221Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_df.groupby(['pred','target']).size().to_frame(name = 'size').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:46:46.895142Z",
     "start_time": "2021-02-09T00:46:46.576169Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = eval_df.groupby(['pred','target']).size().to_frame(name = 'size').reset_index()\n",
    "pd.concat([tmp, pd.Series([\"{:.2f}%\".format(i) for i in tmp.groupby('pred')['size'].apply(lambda x: 100*x/x.sum())]).rename(\"Percentage\")], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T00:46:47.006118Z",
     "start_time": "2021-02-09T00:46:46.897400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tmp = eval_df.groupby(['target', 'pred']).size().to_frame(name = 'size').reset_index()\n",
    "pd.concat([tmp, pd.Series([\"{:.2f}%\".format(i) for i in tmp.groupby('target')['size'].apply(lambda x: 100*x/x.sum())]).rename(\"Percentage\")], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mytest] *",
   "language": "python",
   "name": "conda-env-mytest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "173.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
